{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources if not already present\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Custom stop words or subject-specific common words\n",
    "subject_common_words = {\n",
    "    \"MEDI\": [\"medicine\", \"clinical\", \"health\", \"patient\"],\n",
    "    \"ENGI\": [\"engineering\", \"design\", \"system\"],\n",
    "    \"CHEM\": [\"chemistry\", \"reaction\", \"chemical\"],\n",
    "    \"BIOC\": [\"biochemistry\", \"protein\", \"enzyme\"],\n",
    "    # Add more subject-specific common words as needed\n",
    "}\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Function to remove stopwords and common subject-specific terms\n",
    "def preprocess_text(text, subject_areas):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = []\n",
    "    \n",
    "    # Collect common words for all subject areas\n",
    "    for area in subject_areas:\n",
    "        custom_stop_words.extend(subject_common_words.get(area, []))\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words and word not in custom_stop_words\n",
    "    ]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function to extract keywords using TF-IDF\n",
    "def extract_keywords(corpus, n_keywords=5):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # Adjust based on your dataset size\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        stop_words='english'\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        indices = row.indices\n",
    "        scores = row.data\n",
    "        top_indices = indices[scores.argsort()[-n_keywords:]]  # Get indices of top keywords\n",
    "        top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "        keywords.append(top_keywords)\n",
    "    return keywords\n",
    "\n",
    "# Load data\n",
    "input_file = \"data.csv\"  # Replace with your CSV file path\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure required columns exist\n",
    "if not all(col in data.columns for col in ['title', 'abstract', 'subjectArea']):\n",
    "    raise ValueError(\"CSV must contain 'title', 'abstract', and 'subjectArea' columns.\")\n",
    "\n",
    "# Preprocess text and combine title and abstract\n",
    "data['cleaned_text'] = data.apply(\n",
    "    lambda row: preprocess_text(\n",
    "        clean_text(f\"{row['title']} {row['abstract']}\"), \n",
    "        str(row['subjectArea']).split(';')  # Split multiple subject areas\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Generate keywords for each row\n",
    "corpus = data['cleaned_text'].tolist()\n",
    "data['extracted_keywords'] = extract_keywords(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_file = \"data_with_added_keywords.csv\"\n",
    "data.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data saved to {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
